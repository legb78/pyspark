{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time \n",
    "import random\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import sum as spark_sum, avg, col, round\n",
    "from pyspark.sql.functions import when, isnan, count, rand, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col, rank\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration pour Windows\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['HADOOP_HOME'] = os.path.abspath('.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimpleApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"300\") \\\n",
    "    .config(\"spark.network.timeout\", \"300s\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "rdd_squared = rdd.map(lambda x: x ** 2)\n",
    "print(rdd_squared.collect())  # Result: [1, 4, 9, 16, 25]\n",
    "\n",
    "# Stop SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCICE 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even numbers: [2, 4, 6, 8, 10]\n",
      "Squares of even numbers: [4, 16, 36, 64, 100]\n",
      "Sum of squares of even numbers: 220\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create an RDD from a list of integers\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# 1. Filter even numbers\n",
    "even_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "print(\"Even numbers:\", even_rdd.collect())\n",
    "\n",
    "# 2. Calculate square of each filtered number\n",
    "squared_rdd = even_rdd.map(lambda x: x ** 2)\n",
    "print(\"Squares of even numbers:\", squared_rdd.collect())\n",
    "\n",
    "# 3. Calculate the sum\n",
    "sum_result = squared_rdd.reduce(lambda x, y: x + y)\n",
    "print(\"Sum of squares of even numbers:\", sum_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCICE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_data = [\n",
    "    (\"John Doe\", \"IT\", 65000),\n",
    "    (\"Jane Smith\", \"HR\", 45000),\n",
    "    (\"Robert Johnson\", \"Finance\", 75000),\n",
    "    (\"Maria Garcia\", \"IT\", 55000),\n",
    "    (\"James Brown\", \"Marketing\", 48000),\n",
    "    (\"Emily Davis\", \"Finance\", 62000),\n",
    "    (\"Michael Wilson\", \"HR\", 51000),\n",
    "    (\"Sarah Thompson\", \"Marketing\", 49000)\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), nullable=False),\n",
    "    StructField(\"department\", StringType(), nullable=False),\n",
    "    StructField(\"salary\", IntegerType(), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Schema:\n",
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- department: string (nullable = false)\n",
      " |-- salary: integer (nullable = false)\n",
      "\n",
      "\n",
      "Employees with salary > 50000:\n",
      "+--------------+----------+------+\n",
      "|          name|department|salary|\n",
      "+--------------+----------+------+\n",
      "|      John Doe|        IT| 65000|\n",
      "|Robert Johnson|   Finance| 75000|\n",
      "|  Maria Garcia|        IT| 55000|\n",
      "|   Emily Davis|   Finance| 62000|\n",
      "|Michael Wilson|        HR| 51000|\n",
      "+--------------+----------+------+\n",
      "\n",
      "\n",
      "Average Salary by Department:\n",
      "+----------+----------+\n",
      "|department|avg_salary|\n",
      "+----------+----------+\n",
      "|        IT|   60000.0|\n",
      "|        HR|   48000.0|\n",
      "|   Finance|   68500.0|\n",
      "| Marketing|   48500.0|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_employees = spark.createDataFrame(employee_data, schema=schema)\n",
    "\n",
    "# 1. Display the schema\n",
    "print(\"DataFrame Schema:\")\n",
    "df_employees.printSchema()\n",
    "\n",
    "# 2. Filter employees with salary > 50000\n",
    "high_salary_df = df_employees.filter(col(\"salary\") > 50000)\n",
    "print(\"\\nEmployees with salary > 50000:\")\n",
    "high_salary_df.show()\n",
    "\n",
    "# 3. Calculate average salary by department\n",
    "avg_salary_by_dept = df_employees.groupBy(\"department\").agg(avg(\"salary\").alias(\"avg_salary\"))\n",
    "print(\"\\nAverage Salary by Department:\")\n",
    "avg_salary_by_dept.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCICE 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 5 premières lignes du DataFrame:\n",
      "+---+--------------------+-----------+-------+\n",
      "| id|                name|   category|  price|\n",
      "+---+--------------------+-----------+-------+\n",
      "|  1|  Laptop Dell XPS 15|Electronics|1299.99|\n",
      "|  2|       iPhone 13 Pro|Electronics| 999.99|\n",
      "|  3|Chaise de bureau ...|  Furniture|  249.5|\n",
      "|  4|Table de salle à ...|  Furniture| 399.95|\n",
      "|  5|Monitor LG UltraWide|Electronics| 349.99|\n",
      "+---+--------------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Produits de la catégorie Electronics:\n",
      "+---+--------------------+-----------+-------+\n",
      "| id|                name|   category|  price|\n",
      "+---+--------------------+-----------+-------+\n",
      "|  1|  Laptop Dell XPS 15|Electronics|1299.99|\n",
      "|  2|       iPhone 13 Pro|Electronics| 999.99|\n",
      "|  5|Monitor LG UltraWide|Electronics| 349.99|\n",
      "|  8|Casque audio sans...|Electronics| 199.95|\n",
      "| 10|Tablette Samsung ...|Electronics| 499.99|\n",
      "| 12|  Enceinte Bluetooth|Electronics|  79.99|\n",
      "+---+--------------------+-----------+-------+\n",
      "\n",
      "\n",
      "Prix moyen par catégorie:\n",
      "+-----------+----------+\n",
      "|   category|prix_moyen|\n",
      "+-----------+----------+\n",
      "|    Kitchen|    149.99|\n",
      "|       Home|     76.48|\n",
      "|Electronics|    571.65|\n",
      "|  Furniture|    579.29|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df = spark.read.option(\"header\", \"true\") \\\n",
    "                        .option(\"inferSchema\", \"true\") \\\n",
    "                        .csv(\"products.csv\")\n",
    "\n",
    "# 1. Afficher les 5 premières lignes\n",
    "print(\"Les 5 premières lignes du DataFrame:\")\n",
    "products_df.show(5)\n",
    "\n",
    "# 2. Filtrer les produits d'une catégorie spécifique (Electronics)\n",
    "electronics_df = products_df.filter(col(\"category\") == \"Electronics\")\n",
    "print(\"\\nProduits de la catégorie Electronics:\")\n",
    "electronics_df.show()\n",
    "\n",
    "# 3. Calculer le prix moyen par catégorie\n",
    "avg_price_by_category = products_df.groupBy(\"category\") \\\n",
    "                                  .agg(avg(\"price\").alias(\"prix_moyen\"))\n",
    "print(\"\\nPrix moyen par catégorie:\")\n",
    "avg_price_by_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCICE 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame original:\n",
      "+---+--------------------+-----------+-------+\n",
      "| id|                name|   category|  price|\n",
      "+---+--------------------+-----------+-------+\n",
      "|  1|  Laptop Dell XPS 15|Electronics|1299.99|\n",
      "|  2|       iPhone 13 Pro|Electronics| 999.99|\n",
      "|  3|Chaise de bureau ...|  Furniture|  249.5|\n",
      "|  4|Table de salle à ...|  Furniture| 399.95|\n",
      "|  5|Monitor LG UltraWide|Electronics| 349.99|\n",
      "|  6| Lampe de bureau LED|       Home|  59.99|\n",
      "|  7|      Canapé en cuir|  Furniture|  899.0|\n",
      "|  8|Casque audio sans...|Electronics| 199.95|\n",
      "|  9|      Tapis de salon|       Home|  129.5|\n",
      "| 10|Tablette Samsung ...|Electronics| 499.99|\n",
      "| 11|Bureau en bois ma...|  Furniture|  549.0|\n",
      "| 12|  Enceinte Bluetooth|Electronics|  79.99|\n",
      "| 13| Plante artificielle|       Home|  39.95|\n",
      "| 14|Cafetière automat...|    Kitchen| 149.99|\n",
      "| 15|      Lit queen size|  Furniture|  799.0|\n",
      "+---+--------------------+-----------+-------+\n",
      "\n",
      "\n",
      "DataFrame avec classification des prix (seuil = 300.0):\n",
      "+---+--------------------+-----------+-------+--------------+\n",
      "| id|                name|   category|  price|price_category|\n",
      "+---+--------------------+-----------+-------+--------------+\n",
      "|  1|  Laptop Dell XPS 15|Electronics|1299.99|         Élevé|\n",
      "|  2|       iPhone 13 Pro|Electronics| 999.99|         Élevé|\n",
      "|  3|Chaise de bureau ...|  Furniture|  249.5|           Bas|\n",
      "|  4|Table de salle à ...|  Furniture| 399.95|         Élevé|\n",
      "|  5|Monitor LG UltraWide|Electronics| 349.99|         Élevé|\n",
      "|  6| Lampe de bureau LED|       Home|  59.99|           Bas|\n",
      "|  7|      Canapé en cuir|  Furniture|  899.0|         Élevé|\n",
      "|  8|Casque audio sans...|Electronics| 199.95|           Bas|\n",
      "|  9|      Tapis de salon|       Home|  129.5|           Bas|\n",
      "| 10|Tablette Samsung ...|Electronics| 499.99|         Élevé|\n",
      "| 11|Bureau en bois ma...|  Furniture|  549.0|         Élevé|\n",
      "| 12|  Enceinte Bluetooth|Electronics|  79.99|           Bas|\n",
      "| 13| Plante artificielle|       Home|  39.95|           Bas|\n",
      "| 14|Cafetière automat...|    Kitchen| 149.99|           Bas|\n",
      "| 15|      Lit queen size|  Furniture|  799.0|         Élevé|\n",
      "+---+--------------------+-----------+-------+--------------+\n",
      "\n",
      "\n",
      "Répartition des produits par catégorie de prix:\n",
      "DataFrame original:\n",
      "+---+--------------------+-----------+-------+\n",
      "| id|                name|   category|  price|\n",
      "+---+--------------------+-----------+-------+\n",
      "|  1|  Laptop Dell XPS 15|Electronics|1299.99|\n",
      "|  2|       iPhone 13 Pro|Electronics| 999.99|\n",
      "|  3|Chaise de bureau ...|  Furniture|  249.5|\n",
      "|  4|Table de salle à ...|  Furniture| 399.95|\n",
      "|  5|Monitor LG UltraWide|Electronics| 349.99|\n",
      "|  6| Lampe de bureau LED|       Home|  59.99|\n",
      "|  7|      Canapé en cuir|  Furniture|  899.0|\n",
      "|  8|Casque audio sans...|Electronics| 199.95|\n",
      "|  9|      Tapis de salon|       Home|  129.5|\n",
      "| 10|Tablette Samsung ...|Electronics| 499.99|\n",
      "| 11|Bureau en bois ma...|  Furniture|  549.0|\n",
      "| 12|  Enceinte Bluetooth|Electronics|  79.99|\n",
      "| 13| Plante artificielle|       Home|  39.95|\n",
      "| 14|Cafetière automat...|    Kitchen| 149.99|\n",
      "| 15|      Lit queen size|  Furniture|  799.0|\n",
      "+---+--------------------+-----------+-------+\n",
      "\n",
      "\n",
      "DataFrame avec classification des prix (seuil = 300.0):\n",
      "+---+--------------------+-----------+-------+--------------+\n",
      "| id|                name|   category|  price|price_category|\n",
      "+---+--------------------+-----------+-------+--------------+\n",
      "|  1|  Laptop Dell XPS 15|Electronics|1299.99|         Élevé|\n",
      "|  2|       iPhone 13 Pro|Electronics| 999.99|         Élevé|\n",
      "|  3|Chaise de bureau ...|  Furniture|  249.5|           Bas|\n",
      "|  4|Table de salle à ...|  Furniture| 399.95|         Élevé|\n",
      "|  5|Monitor LG UltraWide|Electronics| 349.99|         Élevé|\n",
      "|  6| Lampe de bureau LED|       Home|  59.99|           Bas|\n",
      "|  7|      Canapé en cuir|  Furniture|  899.0|         Élevé|\n",
      "|  8|Casque audio sans...|Electronics| 199.95|           Bas|\n",
      "|  9|      Tapis de salon|       Home|  129.5|           Bas|\n",
      "| 10|Tablette Samsung ...|Electronics| 499.99|         Élevé|\n",
      "| 11|Bureau en bois ma...|  Furniture|  549.0|         Élevé|\n",
      "| 12|  Enceinte Bluetooth|Electronics|  79.99|           Bas|\n",
      "| 13| Plante artificielle|       Home|  39.95|           Bas|\n",
      "| 14|Cafetière automat...|    Kitchen| 149.99|           Bas|\n",
      "| 15|      Lit queen size|  Furniture|  799.0|         Élevé|\n",
      "+---+--------------------+-----------+-------+--------------+\n",
      "\n",
      "\n",
      "Répartition des produits par catégorie de prix:\n"
     ]
    }
   ],
   "source": [
    "products_df = spark.read.option(\"header\", \"true\") \\\n",
    "                        .option(\"inferSchema\", \"true\") \\\n",
    "                        .csv(\"products.csv\")\n",
    "\n",
    "# Afficher le DataFrame original\n",
    "print(\"DataFrame original:\")\n",
    "products_df.show()\n",
    "\n",
    "# Définir un seuil de prix\n",
    "PRICE_THRESHOLD = 300.0\n",
    "\n",
    "# Définir une fonction Python pour classifier les prix\n",
    "def classify_price(price):\n",
    "    if price > PRICE_THRESHOLD:\n",
    "        return \"Élevé\"\n",
    "    else:\n",
    "        return \"Bas\"\n",
    "\n",
    "# Enregistrer la fonction comme UDF (User Defined Function)\n",
    "price_classifier_udf = udf(classify_price, StringType())\n",
    "\n",
    "# Appliquer l'UDF au DataFrame pour créer une nouvelle colonne\n",
    "products_with_classification = products_df.withColumn(\n",
    "    \"price_category\", \n",
    "    price_classifier_udf(col(\"price\"))\n",
    ")\n",
    "\n",
    "# Afficher le DataFrame avec la nouvelle colonne\n",
    "print(\"\\nDataFrame avec classification des prix (seuil = {}):\".format(PRICE_THRESHOLD))\n",
    "products_with_classification.show()\n",
    "\n",
    "# Compter le nombre de produits par catégorie de prix\n",
    "print(\"\\nRépartition des produits par catégorie de prix:\")\n",
    "products_df = spark.read.option(\"header\", \"true\") \\\n",
    "                        .option(\"inferSchema\", \"true\") \\\n",
    "                        .csv(\"products.csv\")\n",
    "\n",
    "# Afficher le DataFrame original\n",
    "print(\"DataFrame original:\")\n",
    "products_df.show()\n",
    "\n",
    "# Définir un seuil de prix\n",
    "PRICE_THRESHOLD = 300.0\n",
    "\n",
    "# Définir une fonction Python pour classifier les prix\n",
    "def classify_price(price):\n",
    "    if price > PRICE_THRESHOLD:\n",
    "        return \"Élevé\"\n",
    "    else:\n",
    "        return \"Bas\"\n",
    "\n",
    "# Enregistrer la fonction comme UDF (User Defined Function)\n",
    "price_classifier_udf = udf(classify_price, StringType())\n",
    "\n",
    "# Appliquer l'UDF au DataFrame pour créer une nouvelle colonne\n",
    "products_with_classification = products_df.withColumn(\n",
    "    \"price_category\", \n",
    "    price_classifier_udf(col(\"price\"))\n",
    ")\n",
    "\n",
    "# Afficher le DataFrame avec la nouvelle colonne\n",
    "print(\"\\nDataFrame avec classification des prix (seuil = {}):\".format(PRICE_THRESHOLD))\n",
    "products_with_classification.show()\n",
    "\n",
    "# Compter le nombre de produits par catégorie de prix\n",
    "print(\"\\nRépartition des produits par catégorie de prix:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCICE 5 : Jointure de Deux DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame des commandes:\n",
      "+--------+-----------+------+-------------------+\n",
      "|order_id|customer_id|amount|         order_date|\n",
      "+--------+-----------+------+-------------------+\n",
      "|    1001|        101| 150.5|2023-01-15 00:00:00|\n",
      "|    1002|        103|299.99|2023-01-16 00:00:00|\n",
      "|    1003|        102| 85.75|2023-01-18 00:00:00|\n",
      "|    1004|        101| 220.0|2023-01-20 00:00:00|\n",
      "|    1005|        105|1250.0|2023-01-22 00:00:00|\n",
      "|    1006|        104| 65.99|2023-01-25 00:00:00|\n",
      "|    1007|        103| 430.5|2023-01-28 00:00:00|\n",
      "|    1008|        102|199.99|2023-02-01 00:00:00|\n",
      "|    1009|        101|175.25|2023-02-05 00:00:00|\n",
      "|    1010|        106| 340.0|2023-02-10 00:00:00|\n",
      "+--------+-----------+------+-------------------+\n",
      "\n",
      "\n",
      "DataFrame des clients:\n",
      "+-----------+----------------+-------+--------------------+\n",
      "|customer_id|            name|country|               email|\n",
      "+-----------+----------------+-------+--------------------+\n",
      "|        101|    Marie Dupont| France|marie.dupont@exam...|\n",
      "|        102|     Jean Martin| France|jean.martin@examp...|\n",
      "|        103|    Emma Johnson|     UK|  emma.j@example.com|\n",
      "|        104|Carlos Rodriguez|  Spain|carlos.r@example.com|\n",
      "|        105|    Hans Schmidt|Germany|  hans.s@example.com|\n",
      "|        107|Alessandro Rossi|  Italy|alessandro.r@exam...|\n",
      "+-----------+----------------+-------+--------------------+\n",
      "\n",
      "\n",
      "Résultat de la jointure interne:\n",
      "+--------+------+-------------------+-----------+----------------+-------+\n",
      "|order_id|amount|         order_date|customer_id|            name|country|\n",
      "+--------+------+-------------------+-----------+----------------+-------+\n",
      "|    1001| 150.5|2023-01-15 00:00:00|        101|    Marie Dupont| France|\n",
      "|    1002|299.99|2023-01-16 00:00:00|        103|    Emma Johnson|     UK|\n",
      "|    1003| 85.75|2023-01-18 00:00:00|        102|     Jean Martin| France|\n",
      "|    1004| 220.0|2023-01-20 00:00:00|        101|    Marie Dupont| France|\n",
      "|    1005|1250.0|2023-01-22 00:00:00|        105|    Hans Schmidt|Germany|\n",
      "|    1006| 65.99|2023-01-25 00:00:00|        104|Carlos Rodriguez|  Spain|\n",
      "|    1007| 430.5|2023-01-28 00:00:00|        103|    Emma Johnson|     UK|\n",
      "|    1008|199.99|2023-02-01 00:00:00|        102|     Jean Martin| France|\n",
      "|    1009|175.25|2023-02-05 00:00:00|        101|    Marie Dupont| France|\n",
      "+--------+------+-------------------+-----------+----------------+-------+\n",
      "\n",
      "\n",
      "Résultat de la jointure gauche (LEFT JOIN):\n",
      "+--------+------+-------------------+-----------+----------------+-------+\n",
      "|order_id|amount|         order_date|customer_id|            name|country|\n",
      "+--------+------+-------------------+-----------+----------------+-------+\n",
      "|    1001| 150.5|2023-01-15 00:00:00|        101|    Marie Dupont| France|\n",
      "|    1002|299.99|2023-01-16 00:00:00|        103|    Emma Johnson|     UK|\n",
      "|    1003| 85.75|2023-01-18 00:00:00|        102|     Jean Martin| France|\n",
      "|    1004| 220.0|2023-01-20 00:00:00|        101|    Marie Dupont| France|\n",
      "|    1005|1250.0|2023-01-22 00:00:00|        105|    Hans Schmidt|Germany|\n",
      "|    1006| 65.99|2023-01-25 00:00:00|        104|Carlos Rodriguez|  Spain|\n",
      "|    1007| 430.5|2023-01-28 00:00:00|        103|    Emma Johnson|     UK|\n",
      "|    1008|199.99|2023-02-01 00:00:00|        102|     Jean Martin| France|\n",
      "|    1009|175.25|2023-02-05 00:00:00|        101|    Marie Dupont| France|\n",
      "|    1010| 340.0|2023-02-10 00:00:00|        106|            NULL|   NULL|\n",
      "+--------+------+-------------------+-----------+----------------+-------+\n",
      "\n",
      "\n",
      "Résultat de la jointure droite (RIGHT JOIN):\n",
      "+--------+------+-------------------+-----------+----------------+-------+\n",
      "|order_id|amount|         order_date|customer_id|            name|country|\n",
      "+--------+------+-------------------+-----------+----------------+-------+\n",
      "|    1009|175.25|2023-02-05 00:00:00|        101|    Marie Dupont| France|\n",
      "|    1004| 220.0|2023-01-20 00:00:00|        101|    Marie Dupont| France|\n",
      "|    1001| 150.5|2023-01-15 00:00:00|        101|    Marie Dupont| France|\n",
      "|    1008|199.99|2023-02-01 00:00:00|        102|     Jean Martin| France|\n",
      "|    1003| 85.75|2023-01-18 00:00:00|        102|     Jean Martin| France|\n",
      "|    1007| 430.5|2023-01-28 00:00:00|        103|    Emma Johnson|     UK|\n",
      "|    1002|299.99|2023-01-16 00:00:00|        103|    Emma Johnson|     UK|\n",
      "|    1006| 65.99|2023-01-25 00:00:00|        104|Carlos Rodriguez|  Spain|\n",
      "|    1005|1250.0|2023-01-22 00:00:00|        105|    Hans Schmidt|Germany|\n",
      "|    NULL|  NULL|               NULL|        107|Alessandro Rossi|  Italy|\n",
      "+--------+------+-------------------+-----------+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger les données des commandes\n",
    "orders_df = spark.read.option(\"header\", \"true\") \\\n",
    "                      .option(\"inferSchema\", \"true\") \\\n",
    "                      .csv(\"orders.csv\")\n",
    "\n",
    "# Charger les données des clients\n",
    "customers_df = spark.read.option(\"header\", \"true\") \\\n",
    "                         .option(\"inferSchema\", \"true\") \\\n",
    "                         .csv(\"customers.csv\")\n",
    "\n",
    "\n",
    "# Afficher les DataFrames originaux\n",
    "print(\"DataFrame des commandes:\")\n",
    "orders_df.show()\n",
    "\n",
    "print(\"\\nDataFrame des clients:\")\n",
    "customers_df.show()\n",
    "\n",
    "# Effectuer une jointure interne (INNER JOIN)\n",
    "inner_join_df = orders_df.join(\n",
    "    customers_df,\n",
    "    orders_df[\"customer_id\"] == customers_df[\"customer_id\"],\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Sélectionner les colonnes pertinentes\n",
    "result_df = inner_join_df.select(\n",
    "    \"order_id\",\n",
    "    \"amount\",\n",
    "    \"order_date\",\n",
    "    customers_df[\"customer_id\"],\n",
    "    \"name\",\n",
    "    \"country\"\n",
    ")\n",
    "\n",
    "# Afficher le résultat de la jointure interne\n",
    "print(\"\\nRésultat de la jointure interne:\")\n",
    "result_df.show()\n",
    "\n",
    "# Effectuer une jointure externe gauche (LEFT JOIN)\n",
    "# Pour inclure les commandes sans correspondance de client\n",
    "left_join_df = orders_df.join(\n",
    "    customers_df,\n",
    "    orders_df[\"customer_id\"] == customers_df[\"customer_id\"],\n",
    "    \"left\"\n",
    ").select(\n",
    "    \"order_id\",\n",
    "    \"amount\",\n",
    "    \"order_date\",\n",
    "    orders_df[\"customer_id\"],\n",
    "    \"name\",\n",
    "    \"country\"\n",
    ")\n",
    "\n",
    "# Afficher le résultat de la jointure gauche\n",
    "print(\"\\nRésultat de la jointure gauche (LEFT JOIN):\")\n",
    "left_join_df.show()\n",
    "\n",
    "# Effectuer une jointure externe droite (RIGHT JOIN)\n",
    "# Pour inclure les clients sans commandes\n",
    "right_join_df = orders_df.join(\n",
    "    customers_df,\n",
    "    orders_df[\"customer_id\"] == customers_df[\"customer_id\"],\n",
    "    \"right\"\n",
    ").select(\n",
    "    \"order_id\",\n",
    "    \"amount\",\n",
    "    \"order_date\",\n",
    "    customers_df[\"customer_id\"],\n",
    "    \"name\", \n",
    "    \"country\"\n",
    ")\n",
    "\n",
    "# Afficher le résultat de la jointure droite\n",
    "print(\"\\nRésultat de la jointure droite (RIGHT JOIN):\")\n",
    "right_join_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCICE 6 : Agrégation et Groupement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montant total des commandes par pays (ordre décroissant):\n",
      "+-------+------------+\n",
      "|country|total_amount|\n",
      "+-------+------------+\n",
      "|Germany|      1250.0|\n",
      "| France|      831.49|\n",
      "|     UK|      730.49|\n",
      "|  Spain|       65.99|\n",
      "+-------+------------+\n",
      "\n",
      "\n",
      "Répartition des montants par pays avec pourcentage:\n",
      "+-------+------------+----------+\n",
      "|country|total_amount|percentage|\n",
      "+-------+------------+----------+\n",
      "|Germany|      1250.0|     43.43|\n",
      "| France|      831.49|     28.89|\n",
      "|     UK|      730.49|     25.38|\n",
      "|  Spain|       65.99|      2.29|\n",
      "+-------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger les données des commandes\n",
    "orders_df = spark.read.option(\"header\", \"true\") \\\n",
    "                      .option(\"inferSchema\", \"true\") \\\n",
    "                      .csv(\"orders.csv\")\n",
    "\n",
    "# Charger les données des clients\n",
    "customers_df = spark.read.option(\"header\", \"true\") \\\n",
    "                         .option(\"inferSchema\", \"true\") \\\n",
    "                         .csv(\"customers.csv\")\n",
    "\n",
    "# Joindre les DataFrames des commandes et des clients\n",
    "joined_df = orders_df.join(\n",
    "    customers_df,\n",
    "    orders_df[\"customer_id\"] == customers_df[\"customer_id\"],\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Calculer le montant total des commandes par pays\n",
    "total_by_country = joined_df.groupBy(\"country\") \\\n",
    "                           .agg(round(spark_sum(\"amount\"), 2).alias(\"total_amount\")) \\\n",
    "                           .orderBy(col(\"total_amount\").desc())\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Montant total des commandes par pays (ordre décroissant):\")\n",
    "total_by_country.show()\n",
    "\n",
    "# Calcul supplémentaire : Pourcentage du montant total par pays\n",
    "total_amount = joined_df.agg(spark_sum(\"amount\")).collect()[0][0]\n",
    "\n",
    "# Ajouter une colonne de pourcentage\n",
    "result_with_percentage = total_by_country.withColumn(\n",
    "    \"percentage\", \n",
    "    round((col(\"total_amount\") / total_amount) * 100, 2)\n",
    ")\n",
    "\n",
    "# Afficher les résultats avec pourcentage\n",
    "print(\"\\nRépartition des montants par pays avec pourcentage:\")\n",
    "result_with_percentage.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCICE 7 : Gestion des Valeurs Manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame original:\n",
      "+---+---------------+----+------+----------+----------------+\n",
      "| id|           name| age|salary|department|experience_years|\n",
      "+---+---------------+----+------+----------+----------------+\n",
      "|  1|    Jean Dupont|  35| 55000|        IT|            10.0|\n",
      "|  2|   Marie Martin|  28|  NULL| Marketing|             5.0|\n",
      "|  3|  Pierre Durand|NULL| 48000|   Finance|            NULL|\n",
      "|  4|Sophie Lefebvre|  42| 62000|      NULL|            12.0|\n",
      "|  5|           NULL|  31| 44000|        HR|             7.0|\n",
      "|  6|     Marc Petit|  45|  NULL|        IT|            15.0|\n",
      "|  7|  Claire Moreau|  33| 51000| Marketing|            NULL|\n",
      "|  8|   Thomas Leroy|NULL| 38000|   Finance|             4.0|\n",
      "|  9|     Julie Roux|  29|  NULL|        HR|            NULL|\n",
      "| 10|   Paul Bernard|  39| 58000|      NULL|            11.0|\n",
      "+---+---------------+----+------+----------+----------------+\n",
      "\n",
      "\n",
      "Nombre de valeurs manquantes par colonne:\n",
      "+---+----+---+------+----------+----------------+\n",
      "| id|name|age|salary|department|experience_years|\n",
      "+---+----+---+------+----------+----------------+\n",
      "|  0|   1|  2|     3|         2|               3|\n",
      "+---+----+---+------+----------+----------------+\n",
      "\n",
      "\n",
      "Pourcentage de valeurs manquantes par colonne:\n",
      "id: 0.00%\n",
      "name: 10.00%\n",
      "age: 20.00%\n",
      "salary: 30.00%\n",
      "department: 20.00%\n",
      "experience_years: 30.00%\n",
      "\n",
      "DataFrame après suppression des lignes avec valeurs manquantes:\n",
      "+---+-----------+---+------+----------+----------------+\n",
      "| id|       name|age|salary|department|experience_years|\n",
      "+---+-----------+---+------+----------+----------------+\n",
      "|  1|Jean Dupont| 35| 55000|        IT|            10.0|\n",
      "+---+-----------+---+------+----------+----------------+\n",
      "\n",
      "Nombre de lignes restantes: 1 sur 10 initialement\n",
      "\n",
      "DataFrame après suppression des lignes avec moins de 4 valeurs non-nulles:\n",
      "+---+---------------+----+------+----------+----------------+\n",
      "| id|           name| age|salary|department|experience_years|\n",
      "+---+---------------+----+------+----------+----------------+\n",
      "|  1|    Jean Dupont|  35| 55000|        IT|            10.0|\n",
      "|  2|   Marie Martin|  28|  NULL| Marketing|             5.0|\n",
      "|  3|  Pierre Durand|NULL| 48000|   Finance|            NULL|\n",
      "|  4|Sophie Lefebvre|  42| 62000|      NULL|            12.0|\n",
      "|  5|           NULL|  31| 44000|        HR|             7.0|\n",
      "|  6|     Marc Petit|  45|  NULL|        IT|            15.0|\n",
      "|  7|  Claire Moreau|  33| 51000| Marketing|            NULL|\n",
      "|  8|   Thomas Leroy|NULL| 38000|   Finance|             4.0|\n",
      "|  9|     Julie Roux|  29|  NULL|        HR|            NULL|\n",
      "| 10|   Paul Bernard|  39| 58000|      NULL|            11.0|\n",
      "+---+---------------+----+------+----------+----------------+\n",
      "\n",
      "\n",
      "DataFrame après suppression des lignes avec valeurs manquantes dans 'name' ou 'age':\n",
      "+---+---------------+---+------+----------+----------------+\n",
      "| id|           name|age|salary|department|experience_years|\n",
      "+---+---------------+---+------+----------+----------------+\n",
      "|  1|    Jean Dupont| 35| 55000|        IT|            10.0|\n",
      "|  2|   Marie Martin| 28|  NULL| Marketing|             5.0|\n",
      "|  4|Sophie Lefebvre| 42| 62000|      NULL|            12.0|\n",
      "|  6|     Marc Petit| 45|  NULL|        IT|            15.0|\n",
      "|  7|  Claire Moreau| 33| 51000| Marketing|            NULL|\n",
      "|  9|     Julie Roux| 29|  NULL|        HR|            NULL|\n",
      "| 10|   Paul Bernard| 39| 58000|      NULL|            11.0|\n",
      "+---+---------------+---+------+----------+----------------+\n",
      "\n",
      "\n",
      "DataFrame après remplacement de certaines valeurs manquantes par des constantes:\n",
      "+---+---------------+----+------+------------+----------------+\n",
      "| id|           name| age|salary|  department|experience_years|\n",
      "+---+---------------+----+------+------------+----------------+\n",
      "|  1|    Jean Dupont|  35| 55000|          IT|            10.0|\n",
      "|  2|   Marie Martin|  28|  NULL|   Marketing|             5.0|\n",
      "|  3|  Pierre Durand|NULL| 48000|     Finance|             0.0|\n",
      "|  4|Sophie Lefebvre|  42| 62000|Non spécifié|            12.0|\n",
      "|  5|        Inconnu|  31| 44000|          HR|             7.0|\n",
      "|  6|     Marc Petit|  45|  NULL|          IT|            15.0|\n",
      "|  7|  Claire Moreau|  33| 51000|   Marketing|             0.0|\n",
      "|  8|   Thomas Leroy|NULL| 38000|     Finance|             4.0|\n",
      "|  9|     Julie Roux|  29|  NULL|          HR|             0.0|\n",
      "| 10|   Paul Bernard|  39| 58000|Non spécifié|            11.0|\n",
      "+---+---------------+----+------+------------+----------------+\n",
      "\n",
      "\n",
      "Valeurs moyennes calculées:\n",
      "age: 35.25\n",
      "salary: 50857.14\n",
      "experience_years: 9.14\n",
      "\n",
      "DataFrame après remplacement des valeurs numériques manquantes par la moyenne:\n",
      "+---+---------------+---+------+----------+-----------------+\n",
      "| id|           name|age|salary|department| experience_years|\n",
      "+---+---------------+---+------+----------+-----------------+\n",
      "|  1|    Jean Dupont| 35| 55000|        IT|             10.0|\n",
      "|  2|   Marie Martin| 28| 50857| Marketing|              5.0|\n",
      "|  3|  Pierre Durand| 35| 48000|   Finance|9.142857142857142|\n",
      "|  4|Sophie Lefebvre| 42| 62000|      NULL|             12.0|\n",
      "|  5|           NULL| 31| 44000|        HR|              7.0|\n",
      "|  6|     Marc Petit| 45| 50857|        IT|             15.0|\n",
      "|  7|  Claire Moreau| 33| 51000| Marketing|9.142857142857142|\n",
      "|  8|   Thomas Leroy| 35| 38000|   Finance|              4.0|\n",
      "|  9|     Julie Roux| 29| 50857|        HR|9.142857142857142|\n",
      "| 10|   Paul Bernard| 39| 58000|      NULL|             11.0|\n",
      "+---+---------------+---+------+----------+-----------------+\n",
      "\n",
      "\n",
      "Utilisation de l'imputation ML:\n",
      "DataFrame après imputation ML des valeurs numériques:\n",
      "+---+----+------+----------------+-----------+--------------+------------------------+\n",
      "| id| age|salary|experience_years|age_imputed|salary_imputed|experience_years_imputed|\n",
      "+---+----+------+----------------+-----------+--------------+------------------------+\n",
      "|  1|  35| 55000|            10.0|         35|         55000|                    10.0|\n",
      "|  2|  28|  NULL|             5.0|         28|         50857|                     5.0|\n",
      "|  3|NULL| 48000|            NULL|         35|         48000|       9.142857142857142|\n",
      "|  4|  42| 62000|            12.0|         42|         62000|                    12.0|\n",
      "|  5|  31| 44000|             7.0|         31|         44000|                     7.0|\n",
      "|  6|  45|  NULL|            15.0|         45|         50857|                    15.0|\n",
      "|  7|  33| 51000|            NULL|         33|         51000|       9.142857142857142|\n",
      "|  8|NULL| 38000|             4.0|         35|         38000|                     4.0|\n",
      "|  9|  29|  NULL|            NULL|         29|         50857|       9.142857142857142|\n",
      "| 10|  39| 58000|            11.0|         39|         58000|                    11.0|\n",
      "+---+----+------+----------------+-----------+--------------+------------------------+\n",
      "\n",
      "\n",
      "DataFrame après application d'une stratégie combinée:\n",
      "+---+---------------+---+------+------------+-----------------+\n",
      "| id|           name|age|salary|  department| experience_years|\n",
      "+---+---------------+---+------+------------+-----------------+\n",
      "|  1|    Jean Dupont| 35| 55000|          IT|             10.0|\n",
      "|  2|   Marie Martin| 28| 50857|   Marketing|              5.0|\n",
      "|  3|  Pierre Durand| 35| 48000|     Finance|9.142857142857142|\n",
      "|  4|Sophie Lefebvre| 42| 62000|Non spécifié|             12.0|\n",
      "|  5|        Inconnu| 31| 44000|          HR|              7.0|\n",
      "|  6|     Marc Petit| 45| 50857|          IT|             15.0|\n",
      "|  7|  Claire Moreau| 33| 51000|   Marketing|9.142857142857142|\n",
      "|  8|   Thomas Leroy| 35| 38000|     Finance|              4.0|\n",
      "|  9|     Julie Roux| 29| 50857|          HR|9.142857142857142|\n",
      "| 10|   Paul Bernard| 39| 58000|Non spécifié|             11.0|\n",
      "+---+---------------+---+------+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger les données avec valeurs manquantes\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "              .option(\"inferSchema\", \"true\") \\\n",
    "              .csv(\"missing_data.csv\")\n",
    "\n",
    "# Afficher le DataFrame original\n",
    "print(\"DataFrame original:\")\n",
    "df.show()\n",
    "\n",
    "# 1. ANALYSER LES VALEURS MANQUANTES\n",
    "\n",
    "# Compter le nombre de valeurs manquantes par colonne\n",
    "print(\"\\nNombre de valeurs manquantes par colonne:\")\n",
    "df.select([count(when(col(c).isNull() | isnan(col(c)), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Calculer le pourcentage de valeurs manquantes par colonne\n",
    "print(\"\\nPourcentage de valeurs manquantes par colonne:\")\n",
    "num_rows = df.count()\n",
    "for column in df.columns:\n",
    "    null_count = df.filter(col(column).isNull()).count()\n",
    "    percentage = (null_count / num_rows) * 100\n",
    "    print(f\"{column}: {percentage:.2f}%\")\n",
    "\n",
    "# 2. DIFFÉRENTES STRATÉGIES POUR GÉRER LES VALEURS MANQUANTES\n",
    "\n",
    "# A. Supprimer les lignes avec des valeurs manquantes\n",
    "df_drop_na = df.na.drop()\n",
    "print(\"\\nDataFrame après suppression des lignes avec valeurs manquantes:\")\n",
    "df_drop_na.show()\n",
    "print(f\"Nombre de lignes restantes: {df_drop_na.count()} sur {num_rows} initialement\")\n",
    "\n",
    "# B. Supprimer les lignes avec au moins X valeurs manquantes\n",
    "df_drop_thresh = df.na.drop(thresh=4)  # Au moins 4 valeurs non-nulles\n",
    "print(\"\\nDataFrame après suppression des lignes avec moins de 4 valeurs non-nulles:\")\n",
    "df_drop_thresh.show()\n",
    "\n",
    "# C. Supprimer les lignes avec des valeurs manquantes seulement dans certaines colonnes\n",
    "df_drop_subset = df.na.drop(subset=[\"name\", \"age\"])\n",
    "print(\"\\nDataFrame après suppression des lignes avec valeurs manquantes dans 'name' ou 'age':\")\n",
    "df_drop_subset.show()\n",
    "\n",
    "# D. Remplacer les valeurs manquantes par une valeur constante\n",
    "df_fill_constant = df.na.fill({\n",
    "    \"name\": \"Inconnu\",\n",
    "    \"department\": \"Non spécifié\",\n",
    "    \"experience_years\": 0\n",
    "})\n",
    "print(\"\\nDataFrame après remplacement de certaines valeurs manquantes par des constantes:\")\n",
    "df_fill_constant.show()\n",
    "\n",
    "# E. Remplacer les valeurs numériques manquantes par la moyenne\n",
    "# Calculer la moyenne pour les colonnes numériques\n",
    "avg_values = {}\n",
    "for column in [\"age\", \"salary\", \"experience_years\"]:\n",
    "    try:\n",
    "        avg = df.agg({column: \"avg\"}).collect()[0][0]\n",
    "        if avg is not None:\n",
    "            avg_values[column] = avg\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\nValeurs moyennes calculées:\")\n",
    "for col, val in avg_values.items():\n",
    "    print(f\"{col}: {val:.2f}\")\n",
    "\n",
    "# Appliquer les moyennes aux valeurs manquantes\n",
    "df_fill_mean = df.na.fill(avg_values)\n",
    "print(\"\\nDataFrame après remplacement des valeurs numériques manquantes par la moyenne:\")\n",
    "df_fill_mean.show()\n",
    "\n",
    "# F. Utiliser l'imputation ML pour les valeurs numériques manquantes\n",
    "print(\"\\nUtilisation de l'imputation ML:\")\n",
    "# Sélectionner uniquement les colonnes numériques\n",
    "numeric_cols = [\"age\", \"salary\", \"experience_years\"]\n",
    "numeric_df = df.select(\"id\", *numeric_cols)\n",
    "\n",
    "# Configurer l'imputer\n",
    "imputer = Imputer(\n",
    "    inputCols=numeric_cols,\n",
    "    outputCols=[f\"{col}_imputed\" for col in numeric_cols]\n",
    ").setStrategy(\"mean\")  # Peut aussi être \"median\" ou \"mode\"\n",
    "\n",
    "# Appliquer l'imputation\n",
    "imputed_df = imputer.fit(numeric_df).transform(numeric_df)\n",
    "print(\"DataFrame après imputation ML des valeurs numériques:\")\n",
    "imputed_df.show()\n",
    "\n",
    "# G. Stratégie combinée : approche complète\n",
    "# 1. Supprimer les lignes où trop de valeurs sont manquantes\n",
    "# 2. Remplacer les valeurs textuelles manquantes par des constantes\n",
    "# 3. Remplacer les valeurs numériques manquantes par la moyenne\n",
    "\n",
    "df_combined = df.na.drop(thresh=3)  # Au moins 3 valeurs non-nulles\n",
    "df_combined = df_combined.na.fill({\n",
    "    \"name\": \"Inconnu\",\n",
    "    \"department\": \"Non spécifié\"\n",
    "})\n",
    "df_combined = df_combined.na.fill(avg_values)  # Remplir avec les moyennes calculées précédemment\n",
    "\n",
    "print(\"\\nDataFrame après application d'une stratégie combinée:\")\n",
    "df_combined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCICE 8 : Partitionnement et Optimisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Génération d'un DataFrame volumineux...\n",
      "DataFrame généré en 0.12 secondes\n",
      "Nombre de lignes: 1000000\n",
      "Schéma du DataFrame:\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- random1: double (nullable = false)\n",
      " |-- random2: double (nullable = false)\n",
      " |-- group: integer (nullable = true)\n",
      " |-- text: string (nullable = false)\n",
      "\n",
      "\n",
      "Nombre de partitions par défaut: 12\n",
      "\n",
      "=========== TESTS DE PERFORMANCE ===========\n",
      "Opération | Nb partitions | Durée (s)\n",
      "----------------------------------------\n",
      "\n",
      "--- Opération: count ---\n",
      "count | 12 (défaut) | 0.1099\n",
      "count | 1 | 0.0660\n",
      "count | 2 | 0.0701\n",
      "count | 4 | 0.0795\n",
      "count | 8 | 0.0899\n",
      "count | 16 | 0.1399\n",
      "count | 32 | 0.2299\n",
      "count | 64 | 0.3198\n",
      "\n",
      "--- Opération: filter ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m df_test \u001b[38;5;241m=\u001b[39m df_large\u001b[38;5;241m.\u001b[39mcache()  \u001b[38;5;66;03m# Mise en cache pour éviter de recalculer le dataframe\u001b[39;00m\n\u001b[0;32m     53\u001b[0m df_test\u001b[38;5;241m.\u001b[39mcount()  \u001b[38;5;66;03m# Force l'évaluation pour mettre en cache\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m _, default_time \u001b[38;5;241m=\u001b[39m \u001b[43mtime_execution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperation\u001b[39m\u001b[38;5;124m\"\u001b[39m: op_name,\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartitions\u001b[39m\u001b[38;5;124m\"\u001b[39m: num_partitions_default,\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m: default_time\n\u001b[0;32m     60\u001b[0m })\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_partitions_default\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (défaut) | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdefault_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m, in \u001b[0;36mtime_execution\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtime_execution\u001b[39m(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      3\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 4\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result, end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[14], line 35\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     30\u001b[0m partition_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Opérations à tester\u001b[39;00m\n\u001b[0;32m     33\u001b[0m operations \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m df: df\u001b[38;5;241m.\u001b[39mcount(),\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m df: df\u001b[38;5;241m.\u001b[39mfilter(\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrandom1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m50\u001b[39m)\u001b[38;5;241m.\u001b[39mcount(),\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup_by\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m df: df\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;241m.\u001b[39mcollect(),\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msort\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m df: df\u001b[38;5;241m.\u001b[39msort(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mcollect(),\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoin\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m df: df\u001b[38;5;241m.\u001b[39mjoin(df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m1000\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m     39\u001b[0m }\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Stocker les résultats\u001b[39;00m\n\u001b[0;32m     42\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "# Fonction utilitaire pour mesurer les performances\n",
    "def time_execution(func, *args, **kwargs):\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "# 1. GÉNÉRATION D'UN GRAND DATAFRAME (1 million de lignes)\n",
    "print(\"Génération d'un DataFrame volumineux...\")\n",
    "\n",
    "# Méthode 1: Utiliser range pour créer un grand DataFrame\n",
    "start_time = time.time()\n",
    "df_large = spark.range(0, 1000000) \\\n",
    "    .withColumn(\"random1\", rand() * 100) \\\n",
    "    .withColumn(\"random2\", rand() * 1000) \\\n",
    "    .withColumn(\"group\", expr(\"CAST(id % 100 AS INT)\")) \\\n",
    "    .withColumn(\"text\", expr(\"CONCAT('text_', CAST(id AS STRING))\"))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"DataFrame généré en {end_time - start_time:.2f} secondes\")\n",
    "print(f\"Nombre de lignes: {df_large.count()}\")\n",
    "print(f\"Schéma du DataFrame:\")\n",
    "df_large.printSchema()\n",
    "\n",
    "# 2. VÉRIFICATION DU NOMBRE DE PARTITIONS PAR DÉFAUT\n",
    "num_partitions_default = df_large.rdd.getNumPartitions()\n",
    "print(f\"\\nNombre de partitions par défaut: {num_partitions_default}\")\n",
    "\n",
    "# 3. BENCHMARK AVEC DIFFÉRENTS NOMBRES DE PARTITIONS\n",
    "partition_sizes = [1, 2, 4, 8, 16, 32, 64]\n",
    "\n",
    "# Opérations à tester\n",
    "operations = {\n",
    "    \"count\": lambda df: df.count(),\n",
    "    \"filter\": lambda df: df.filter(col(\"random1\") > 50).count(),\n",
    "    \"group_by\": lambda df: df.groupBy(\"group\").count().collect(),\n",
    "    \"sort\": lambda df: df.sort(\"random1\").limit(100).collect(),\n",
    "    \"join\": lambda df: df.join(df.select(\"id\", \"random1\").limit(1000), \"id\").limit(100).collect()\n",
    "}\n",
    "\n",
    "# Stocker les résultats\n",
    "results = []\n",
    "\n",
    "print(\"\\n=========== TESTS DE PERFORMANCE ===========\")\n",
    "print(\"Opération | Nb partitions | Durée (s)\")\n",
    "print(\"----------------------------------------\")\n",
    "\n",
    "for op_name, op_func in operations.items():\n",
    "    print(f\"\\n--- Opération: {op_name} ---\")\n",
    "    \n",
    "    # D'abord tester avec le nombre de partitions par défaut\n",
    "    df_test = df_large.cache()  # Mise en cache pour éviter de recalculer le dataframe\n",
    "    df_test.count()  # Force l'évaluation pour mettre en cache\n",
    "    \n",
    "    _, default_time = time_execution(op_func, df_test)\n",
    "    results.append({\n",
    "        \"operation\": op_name,\n",
    "        \"partitions\": num_partitions_default,\n",
    "        \"time\": default_time\n",
    "    })\n",
    "    print(f\"{op_name} | {num_partitions_default} (défaut) | {default_time:.4f}\")\n",
    "    \n",
    "    # Tester avec différents nombres de partitions\n",
    "    for num_parts in partition_sizes:\n",
    "        if num_parts != num_partitions_default:\n",
    "            # Repartitionner le DataFrame\n",
    "            df_repartitioned = df_large.repartition(num_parts).cache()\n",
    "            df_repartitioned.count()  # Force l'évaluation pour mettre en cache\n",
    "            \n",
    "            _, execution_time = time_execution(op_func, df_repartitioned)\n",
    "            results.append({\n",
    "                \"operation\": op_name,\n",
    "                \"partitions\": num_parts,\n",
    "                \"time\": execution_time\n",
    "            })\n",
    "            print(f\"{op_name} | {num_parts} | {execution_time:.4f}\")\n",
    "            \n",
    "            # Libérer la mémoire\n",
    "            df_repartitioned.unpersist()\n",
    "    \n",
    "    # Libérer la mémoire\n",
    "    df_test.unpersist()\n",
    "\n",
    "# 4. ANALYSE ET VISUALISATION DES RÉSULTATS\n",
    "print(\"\\n=========== RÉSUMÉ DES PERFORMANCES ===========\")\n",
    "print(\"Voici les meilleurs nombres de partitions par opération:\")\n",
    "\n",
    "# Trouver le meilleur nombre de partitions pour chaque opération\n",
    "best_partitions = {}\n",
    "for op_name in operations.keys():\n",
    "    op_results = [r for r in results if r[\"operation\"] == op_name]\n",
    "    best_result = min(op_results, key=lambda x: x[\"time\"])\n",
    "    best_partitions[op_name] = best_result\n",
    "    \n",
    "    print(f\"{op_name}: {best_result['partitions']} partitions \" +\n",
    "          f\"({best_result['time']:.4f}s, \" +\n",
    "          f\"{(default_time / best_result['time'] - 1) * 100:.1f}% plus rapide que la valeur par défaut)\")\n",
    "\n",
    "print(\"\\n=========== ENSEIGNEMENTS ===========\")\n",
    "print(\"\"\"\n",
    "1. Le nombre optimal de partitions dépend de:\n",
    "   - La nature de l'opération (count, filter, join, etc.)\n",
    "   - La taille des données\n",
    "   - Les ressources disponibles (CPU cores)\n",
    "\n",
    "2. Tendances générales:\n",
    "   - Opérations de lecture/scan: plus de partitions = meilleur\n",
    "   - Opérations de shuffle (groupBy, join): trop de partitions peut dégrader les performances\n",
    "   - Opérations de tri: dépend de la stratégie de partition\n",
    "\n",
    "3. Bonnes pratiques:\n",
    "   - Partitionnez selon le nombre de cœurs disponibles\n",
    "   - Pour les opérations de shuffle, 2-3x le nombre de cœurs\n",
    "   - Pour les jointures, partitionnez sur la clé de jointure\n",
    "   - Testez différentes configurations pour votre cas d'usage spécifique\n",
    "\"\"\")\n",
    "\n",
    "# Nettoyage\n",
    "df_large.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCICE 9 : Utilisation de Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Products:\n",
      "+---+---------+-----------+-----+\n",
      "| id|     name|   category|price|\n",
      "+---+---------+-----------+-----+\n",
      "|  1|Product A|Electronics|150.0|\n",
      "|  2|Product B|Electronics|200.0|\n",
      "|  3|Product C|  Furniture|300.0|\n",
      "|  4|Product D|  Furniture|400.0|\n",
      "|  5|Product E|Electronics|250.0|\n",
      "+---+---------+-----------+-----+\n",
      "\n",
      "Electronics Products:\n",
      "+---+---------+-----------+-----+\n",
      "| id|     name|   category|price|\n",
      "+---+---------+-----------+-----+\n",
      "|  1|Product A|Electronics|150.0|\n",
      "|  2|Product B|Electronics|200.0|\n",
      "|  5|Product E|Electronics|250.0|\n",
      "+---+---------+-----------+-----+\n",
      "\n",
      "Average Price by Category:\n",
      "+-----------+---------+\n",
      "|   category|avg_price|\n",
      "+-----------+---------+\n",
      "|Electronics|    200.0|\n",
      "|  Furniture|    350.0|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example DataFrame creation (you can replace this with your own DataFrame)\n",
    "data = [\n",
    "    (1, \"Product A\", \"Electronics\", 150.00),\n",
    "    (2, \"Product B\", \"Electronics\", 200.00),\n",
    "    (3, \"Product C\", \"Furniture\", 300.00),\n",
    "    (4, \"Product D\", \"Furniture\", 400.00),\n",
    "    (5, \"Product E\", \"Electronics\", 250.00)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"category\", \"price\"]\n",
    "df_products = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Save DataFrame as a temporary view\n",
    "df_products.createOrReplaceTempView(\"products\")\n",
    "\n",
    "# Execute SQL queries\n",
    "# 1. Select all products\n",
    "all_products = spark.sql(\"SELECT * FROM products\")\n",
    "print(\"All Products:\")\n",
    "all_products.show()\n",
    "\n",
    "# 2. Filter products by category\n",
    "electronics_products = spark.sql(\"SELECT * FROM products WHERE category = 'Electronics'\")\n",
    "print(\"Electronics Products:\")\n",
    "electronics_products.show()\n",
    "\n",
    "# 3. Calculate average price by category\n",
    "avg_price_by_category = spark.sql(\"SELECT category, AVG(price) as avg_price FROM products GROUP BY category\")\n",
    "print(\"Average Price by Category:\")\n",
    "avg_price_by_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCICE 10 : Implémentation d'une Fonction de Fenêtre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 14\u001b[0m\n\u001b[0;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJohn Doe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m60000\u001b[39m),\n\u001b[0;32m      4\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJane Smith\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m50000\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSarah Thompson\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMarketing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m48000\u001b[39m)\n\u001b[0;32m     11\u001b[0m ]\n\u001b[0;32m     13\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepartment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msalary\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 14\u001b[0m df_employees \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Create a window specification\u001b[39;00m\n\u001b[0;32m     17\u001b[0m window_spec \u001b[38;5;241m=\u001b[39m Window\u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepartment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39morderBy(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msalary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc())\n",
      "File \u001b[1;32mc:\\Users\\ouaza\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m   1442\u001b[0m     )\n\u001b[1;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ouaza\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[1;32mc:\\Users\\ouaza\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:1116\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;66;03m# convert python objects to sql data\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m internal_data \u001b[38;5;241m=\u001b[39m [struct\u001b[38;5;241m.\u001b[39mtoInternal(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tupled_data]\n\u001b[1;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minternal_data\u001b[49m\u001b[43m)\u001b[49m, struct\n",
      "File \u001b[1;32mc:\\Users\\ouaza\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:783\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[1;34m(self, c, numSlices)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallelize\u001b[39m(\u001b[38;5;28mself\u001b[39m, c: Iterable[T], numSlices: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RDD[T]:\n\u001b[0;32m    752\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;124;03m    Distribute a local Python collection to form an RDD. Using range\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;124;03m    is recommended if the input represents a range for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;124;03m    [['a'], ['b', 'c']]\u001b[39;00m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 783\u001b[0m     numSlices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(numSlices) \u001b[38;5;28;01mif\u001b[39;00m numSlices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefaultParallelism\u001b[49m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, \u001b[38;5;28mrange\u001b[39m):\n\u001b[0;32m    785\u001b[0m         size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(c)\n",
      "File \u001b[1;32mc:\\Users\\ouaza\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:630\u001b[0m, in \u001b[0;36mSparkContext.defaultParallelism\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefaultParallelism\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    620\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;124;03m    Default level of parallelism to use when not given by user (e.g. for reduce tasks)\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m()\u001b[38;5;241m.\u001b[39mdefaultParallelism()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example DataFrame creation (you can replace this with your own DataFrame)\n",
    "data = [\n",
    "    (\"John Doe\", \"IT\", 60000),\n",
    "    (\"Jane Smith\", \"HR\", 50000),\n",
    "    (\"Robert Johnson\", \"Finance\", 70000),\n",
    "    (\"Maria Garcia\", \"IT\", 80000),\n",
    "    (\"James Brown\", \"Marketing\", 45000),\n",
    "    (\"Emily Davis\", \"Finance\", 60000),\n",
    "    (\"Michael Wilson\", \"HR\", 55000),\n",
    "    (\"Sarah Thompson\", \"Marketing\", 48000)\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"department\", \"salary\"]\n",
    "df_employees = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Create a window specification\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "# Calculate the rank of each employee within their department\n",
    "df_ranked = df_employees.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "# Show the results\n",
    "print(\"Employee Ranking by Salary within Each Department:\")\n",
    "df_ranked.show()\n",
    "\n",
    "# Stop SparkSession\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
